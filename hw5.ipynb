{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1499fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lad1chka/HSE/DL/HSE_DL4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deee3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_out = F.relu(self.conv(embedded))\n",
    "        pooled = self.pool(conv_out).squeeze(2)\n",
    "        dropped = self.dropout(pooled)\n",
    "        return self.fc(dropped)\n",
    "\n",
    "class ConceptualTAM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_topics, num_aspects, num_classes):\n",
    "        super(ConceptualTAM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.topic_conv = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n",
    "        self.topic_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.topic_linear = nn.Linear(128, num_topics)\n",
    "        \n",
    "        self.aspect_rnn = nn.GRU(embedding_dim, 64, batch_first=True, bidirectional=True)\n",
    "        self.aspect_linear = nn.Linear(128, num_aspects)\n",
    "        \n",
    "        self.classifier = nn.Linear(num_topics + num_aspects, num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        embedded_permuted = embedded.permute(0, 2, 1)\n",
    "        topic_features = F.relu(self.topic_conv(embedded_permuted))\n",
    "        topic_pooled = self.topic_pool(topic_features).squeeze(2)\n",
    "        topic_output = F.relu(self.topic_linear(topic_pooled))\n",
    "        \n",
    "        aspect_out, _ = self.aspect_rnn(embedded)\n",
    "        aspect_pooled = torch.mean(aspect_out, dim=1)\n",
    "        aspect_output = F.relu(self.aspect_linear(aspect_pooled))\n",
    "        \n",
    "        combined = torch.cat((topic_output, aspect_output), dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        return logits, topic_output, aspect_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071790fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    dataset = load_dataset('imdb')\n",
    "    \n",
    "    train_data = dataset['train']\n",
    "    test_data = dataset['test']\n",
    "    \n",
    "    train_indices = list(range(len(train_data)))\n",
    "    test_indices = list(range(len(test_data)))\n",
    "    \n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(train_indices)\n",
    "    random.shuffle(test_indices)\n",
    "    \n",
    "    train_sampled_indices = train_indices[:4500]\n",
    "    test_sampled_indices = test_indices[:1500]\n",
    "    \n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx in train_sampled_indices:\n",
    "        all_texts.append(train_data[idx]['text'])\n",
    "        all_labels.append(train_data[idx]['label'])\n",
    "    \n",
    "    for idx in test_sampled_indices:\n",
    "        all_texts.append(test_data[idx]['text'])\n",
    "        all_labels.append(test_data[idx]['label'])\n",
    "    \n",
    "    combined = list(zip(all_texts, all_labels))\n",
    "    random.shuffle(combined)\n",
    "    all_texts, all_labels = zip(*combined)\n",
    "    \n",
    "    print(f\"Total dataset size: {len(all_texts)}\")\n",
    "    print(f\"Class 0: {sum(1 for l in all_labels if l == 0)} samples\")\n",
    "    print(f\"Class 1: {sum(1 for l in all_labels if l == 1)} samples\")\n",
    "    \n",
    "    max_len = 100\n",
    "    vocab_size = 2000\n",
    "    \n",
    "    from collections import Counter\n",
    "    vocab = Counter()\n",
    "    for text in all_texts:\n",
    "        vocab.update(text.lower().split())\n",
    "    \n",
    "    word_to_idx = {word: i+1 for i, (word, _) in enumerate(vocab.most_common(vocab_size-1))}\n",
    "    word_to_idx['<PAD>'] = 0\n",
    "    \n",
    "    encoded_texts = []\n",
    "    for text in all_texts:\n",
    "        tokens = text.lower().split()[:max_len]\n",
    "        encoded = [word_to_idx.get(token, 0) for token in tokens]\n",
    "        encoded += [0] * (max_len - len(encoded))\n",
    "        encoded_texts.append(encoded)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        encoded_texts, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain size: {len(X_train)}\")\n",
    "    print(f\"Test size: {len(X_test)}\")\n",
    "    print(f\"Train - Class 0: {sum(1 for l in y_train if l == 0)}, Class 1: {sum(1 for l in y_train if l == 1)}\")\n",
    "    print(f\"Test - Class 0: {sum(1 for l in y_test if l == 0)}, Class 1: {sum(1 for l in y_test if l == 1)}\")\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.long),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.long),\n",
    "        torch.tensor(y_test, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset, vocab_size, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de201ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=5, model_name=\"Model\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_texts, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if model_name == \"TAM\":\n",
    "                outputs, _, _ = model(batch_texts)\n",
    "            else:\n",
    "                outputs = model(batch_texts)\n",
    "            \n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in test_loader:\n",
    "                if model_name == \"TAM\":\n",
    "                    outputs, _, _ = model(batch_texts)\n",
    "                else:\n",
    "                    outputs = model(batch_texts)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        print(f\"{model_name} - Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95bf731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_aspect(model, test_loader):\n",
    "    model.eval()\n",
    "    topic_vectors, aspect_vectors = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_texts, _ in test_loader:\n",
    "            _, topic, aspect = model(batch_texts)\n",
    "            topic_vectors.append(topic.cpu().numpy())\n",
    "            aspect_vectors.append(aspect.cpu().numpy())\n",
    "    \n",
    "    topic_np = np.vstack(topic_vectors)\n",
    "    aspect_np = np.vstack(aspect_vectors)\n",
    "    \n",
    "    topic_var = topic_np.var(axis=0).mean()\n",
    "    aspect_var = aspect_np.var(axis=0).mean()\n",
    "    \n",
    "    return topic_var, aspect_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690c6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 6000\n",
      "Class 0: 3003 samples\n",
      "Class 1: 2997 samples\n",
      "\n",
      "Train size: 4200\n",
      "Test size: 1800\n",
      "Train - Class 0: 2102, Class 1: 2098\n",
      "Test - Class 0: 901, Class 1: 899\n",
      "Training SimpleCNN...\n",
      "CNN - Epoch 1, Loss: 100.3684, Accuracy: 0.5478, F1: 0.4468\n",
      "CNN - Epoch 2, Loss: 87.1948, Accuracy: 0.6378, F1: 0.6162\n",
      "CNN - Epoch 3, Loss: 81.3784, Accuracy: 0.6944, F1: 0.6915\n",
      "CNN - Epoch 4, Loss: 76.3186, Accuracy: 0.7000, F1: 0.6948\n",
      "CNN - Epoch 5, Loss: 72.6119, Accuracy: 0.7217, F1: 0.7209\n",
      "CNN - Epoch 6, Loss: 69.4396, Accuracy: 0.7222, F1: 0.7185\n",
      "CNN - Epoch 7, Loss: 63.3638, Accuracy: 0.7333, F1: 0.7330\n",
      "CNN - Epoch 8, Loss: 61.3682, Accuracy: 0.7433, F1: 0.7418\n",
      "CNN - Epoch 9, Loss: 57.0598, Accuracy: 0.7456, F1: 0.7449\n",
      "CNN - Epoch 10, Loss: 54.6970, Accuracy: 0.7450, F1: 0.7428\n",
      "CNN - Epoch 11, Loss: 51.1898, Accuracy: 0.7461, F1: 0.7461\n",
      "CNN - Epoch 12, Loss: 46.5939, Accuracy: 0.7439, F1: 0.7418\n",
      "CNN - Epoch 13, Loss: 46.4821, Accuracy: 0.7506, F1: 0.7504\n",
      "CNN - Epoch 14, Loss: 42.2829, Accuracy: 0.7428, F1: 0.7407\n",
      "CNN - Epoch 15, Loss: 40.8359, Accuracy: 0.7417, F1: 0.7404\n",
      "\n",
      "Training TAM...\n",
      "TAM - Epoch 1, Loss: 91.8132, Accuracy: 0.5006, F1: 0.3340\n",
      "TAM - Epoch 2, Loss: 87.1605, Accuracy: 0.6539, F1: 0.6469\n",
      "TAM - Epoch 3, Loss: 80.2048, Accuracy: 0.6583, F1: 0.6415\n",
      "TAM - Epoch 4, Loss: 72.6544, Accuracy: 0.7083, F1: 0.6997\n",
      "TAM - Epoch 5, Loss: 67.5773, Accuracy: 0.7317, F1: 0.7282\n",
      "TAM - Epoch 6, Loss: 61.6643, Accuracy: 0.7533, F1: 0.7532\n",
      "TAM - Epoch 7, Loss: 59.6791, Accuracy: 0.7633, F1: 0.7633\n",
      "\n",
      "Analyzing TAM components...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, vocab_size, max_len = prepare_data()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Training SimpleCNN...\")\n",
    "cnn_model = SimpleCNN(vocab_size, 100, 2)\n",
    "cnn_acc, cnn_f1 = train_model(cnn_model, train_loader, test_loader, epochs=15, model_name=\"CNN\")\n",
    "\n",
    "print(\"\\nTraining TAM...\")\n",
    "tam_model = ConceptualTAM(vocab_size, 100, max_len, num_topics=10, num_aspects=5, num_classes=2)\n",
    "tam_acc, tam_f1 = train_model(tam_model, train_loader, test_loader, epochs=7, model_name=\"TAM\")\n",
    "\n",
    "print(\"\\nAnalyzing TAM components...\")\n",
    "topic_var, aspect_var = evaluate_topic_aspect(tam_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde9fd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON RESULTS\n",
      "============================================================\n",
      "\n",
      "ACCURACY:\n",
      "SimpleCNN: 0.7417\n",
      "TAM:       0.7633\n",
      "Difference: 0.0217 (TAM better)\n",
      "\n",
      "F1:\n",
      "SimpleCNN: 0.7404\n",
      "TAM:       0.7633\n",
      "Difference: 0.0229 (TAM better)\n",
      "\n",
      "TAM:\n",
      "Topic vector variance:  0.000000\n",
      "Aspect vector variance: 2.492475\n",
      "\n",
      "PARAMETERS:\n",
      "SimpleCNN: 238,786 parameters\n",
      "TAM:       304,239 parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nACCURACY:\")\n",
    "print(f\"SimpleCNN: {cnn_acc:.4f}\")\n",
    "print(f\"TAM:       {tam_acc:.4f}\")\n",
    "print(f\"Difference: {abs(cnn_acc - tam_acc):.4f} ({'CNN better' if cnn_acc > tam_acc else 'TAM better'})\")\n",
    "\n",
    "print(f\"\\nF1:\")\n",
    "print(f\"SimpleCNN: {cnn_f1:.4f}\")\n",
    "print(f\"TAM:       {tam_f1:.4f}\")\n",
    "print(f\"Difference: {abs(cnn_f1 - tam_f1):.4f} ({'CNN better' if cnn_f1 > tam_f1 else 'TAM better'})\")\n",
    "\n",
    "print(f\"\\nTAM:\")\n",
    "print(f\"Topic vector variance:  {topic_var:.6f}\")\n",
    "print(f\"Aspect vector variance: {aspect_var:.6f}\")\n",
    "\n",
    "print(f\"\\nPARAMETERS:\")\n",
    "cnn_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "tam_params = sum(p.numel() for p in tam_model.parameters())\n",
    "print(f\"SimpleCNN: {cnn_params:,} parameters\")\n",
    "print(f\"TAM:       {tam_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd35e3",
   "metadata": {},
   "source": [
    "В целом, можно сказать, что TAM обучается заметно эффективнее по обеим классическим метрикам, чем базовая CNN, но обучается она в разы дольше, хоть и параметров всего в ~1.5 раза больше. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
